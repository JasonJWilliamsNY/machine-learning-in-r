---
title: "Introduction to Machine Learning in R"
output:
  html_document: default
  html_notebook: default
---

## Introductory words

This is an Introduction to Machine Learning in R, in which you'll learn the basic of unsupervised learning for _pattern recognition_ and supervised learning for _prediction_. At the end of this workshop, we hope that you'll

* appreciate the importance of performing exploratory data analysis (or EDA) before starting to model your data.
* understand the basic of _unsupervised learning_ and know the examples of principal component analysis (PCA) and k-means clustering.
* understand the basics of _supervised learning_ for prediction and the differences between _classification_ and _regression_.
* understand modern machine learning techniques and principles, such as test train split, k-fold cross validation and regularization.
* be able to write code to implement the above techniques and methodologies using R, caret and glmnet.

## Acknowledgements

This material has come from many conversations, workshops and online courses over the years, most notably the work that I have done at DataCamp. Some of the material is similar to material that I developed for DataCamp's [
Supervised Learning with scikit-learn course](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn), on which I collaborated with Andreas MÃ¼ller and Yashas Roy, along with community articles that I have written, such as [Kaggle Tutorial: EDA & Machine Learning](https://www.datacamp.com/community/tutorials/kaggle-machine-learning-eda) & [Experts' Favorite Data Science Techniques](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed). Finally, I found time to develop this material due to the 20% community time that I have at DataCamp and am indebted to them for this.



## Getting set up computationally

First you'll install the necessary packages and then load them.

```{r message=FALSE, warning=FALSE}
# Run this cell to install & load the required packages
# If you need to install them, uncomment the lines of code below
#install.packages("tidyverse")
#install.packages("kernlab")
#install.packages("ddalpha")
#install.packages("caret")
#install.packages("GGally")
#install.packages("gmodels")
#install.packages("glmnet", repos = "http://cran.us.r-project.org")
#install.packages("e1071")


# Load packages
library(tidyverse)
library(kernlab)
library(ddalpha)
library(caret)
library(GGally)
library(gmodels)
library(glmnet)
```



## Loading your data

It's time to import the first dataset that we'll work with, the [Breast Cancer Wisconsin (Diagnostic) Data Set](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29) from the UCI Machine Learning repository.

Do this and check out the first several rows:

```{r message=FALSE, warning=FALSE}
# Load data
df <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
               col_names = FALSE)
# Check out head of dataframe
df %>% head()
```


**Discussion:** What are the variables in the dataset? Follow the link to UCI above to find out.

Before thinking about modeling, have a look at your data. There's no point in throwing a $10^4$ layer convolutional neural network (whatever that means) at your data before you even know what you're dealing with.

You'll first remove the column, which is the unique identifier of each row:

```{r}
# Remove first column 
df <- df[2:31]
# View head
df %>% head()
```

**Question:** How many features are there in this dataset?

**Discussion:** Why did we want to remove the unique identifier?

Now there are too many features to plot so you'll plot the first 5 in a pair-plot:

```{r}
# Pair-plot of first 5 features
ggpairs(df[1:5], aes(colour=X2, alpha=0.4))
```


**Discussion:** What can you see here?

Note that the features have widely varying centers and scales (means and standard deviations) so we'll want center and scale them in some situations. You'll use the caret package for this and you can read more about preprocessing with caret [here](https://topepo.github.io/caret/pre-processing.html#pp).

```{r}
# Center & scale data
ppv <- preProcess(df, method = c("center", "scale"))
df_tr <- predict(ppv, df)
# Summarize first 5 columns
df_tr[1:5] %>% summary()
```


Now plot the centered & scaled features:

```{r}
# Pair-plot of transformed data
ggpairs(df_tr[1:5], aes(colour=X2))
```


**Discussion:** How does this compare to your previous pairplot?

## Unsupervised Learning I: dimensionality reduction

*Machine learning* is the science and art of giving computers the ability to learn to make decisions from data without being explicitly programmed.

*Unsupervised learning*, in essence, is the machine learning task of uncovering hidden patterns and structures from unlabeled data. For example, a business may wish to group its customers into distinct categories based on their purchasing behavior without knowing in advance what these categories maybe. This is known as clustering, one branch of unsupervised learning.

Another form of *unsupervised learning*, is _dimensionality reduction_: in the breast cancer dataset, for example, there are too many features to keep track of. What if we could reduced the number of features yet still keep much of the information? 

**Discussion:** Look at features X3 and X5. Do you think that we could reduce them to one feature and keep much of the information?


Principal component analysis  will extract the features with the largest variance. Here let's take the 1st 2 principal components and plot them, colored by tumour diagnosis.

Aside: *Supervised learning*, which we'll get to soon enough, is the branch of machine learning that involves predicting labels, such as whether a tumour will be *benign* or *malignant*.


```{r}
# PCA on data
ppv_pca <- preProcess(df, method = c("center", "scale", "pca"))
df_pc <- predict(ppv_pca, df)
# Plot 1st 2 principal components
ggplot(df_pc, aes(x = PC1, y = PC2, colour = X2)) + geom_point()
```

**Note:** What PCA essentially does is the following:

1. The first step of PCA is to decorrelate your data and this corresponds to a linear transformation of the vector space your data lie in;
2. The second step is the actual dimension reduction; what is really happening is that your decorrelation step (the first step above) transforms the features into new and uncorrelated features; this second step then chooses the features that contain most of the information about the data (you'll formalize this soon enough).

You can essentially think about PCA as a form of compression and you can read more about PCA [here](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed#pca).

## Unsupervised Learning II: Clustering

One popular technique in unsupervised learning is _clustering_. Essentially, this is the task of grouping your data points, based on something about them, such as closeness in space. What you're going to do is group the tumour data points into two clusters using an algorithm called k-means, which aims to cluster the data in in order to minimize the variances of the clusters.

Cluster your data points using k-means and then we'll compare the results to the actual labels that we know:

```{r}
# k-means
km.out <- kmeans(df[,2:10], centers=2, nstart=20)
summary(km.out)
km.out$cluster
```

Now that you have a cluster for each tumour (clusters 1 and 2), you can see how well they coincide with the labels that you know. To do this you'll use a cool method called cross-tabulation: a cross-tab is a table that allows you to read off how many data points in clusters 1 and 2 were actually benign or malignant. respectively.

Let's do it:


```{r}
# Cross-tab of clustering & known labels
CrossTable(df$X2, km.out$cluster)
```

**Discussion:** How well did the k-means do at clustering the tumour data?


## Supervised Learning I: classification


As mentioned in passing above: *Supervised learning*, is the branch of machine learning that involves predicting labels, such as whether a tumour will be *benign* or *malignant*.

In this section, you'll attempt to predict tumour diagnosis based on geometrical measurements.

**Discussion:** Look at your pair plot above. What would a baseline model there be?

**TO DO:** Build model that predicts diagnosis based on whether $X3 > 15$ or something similar.
```{r}
# Build baseline model
df$pred <- ifelse(df$X3 > 15, "M", "B")
df$pred
```

This is not a great model but it does give us a baseline: any model that we build later needs to perform better than this one.

Whoa: what do we mean by _model performance_ here? There are many _metrics_ to determine model performance and here we'll use _accuracy_, the percentage of the data that the model got correct.

**Note on terminology:**

- The _target variable_ is the one you are trying to predict;
- Other variables are known as _features_ (or _predictor variables_).

We first need to change `df$X2`, the _target variable_, to a factor:

```{r}
# What is the class of X2?
class(df$X2)
# Change it to a factor
df$X2 <- as.factor(df$X2)
# What is the class of X2 now?
class(df$X2)
```

Calculate baseline model accuracy:

```{r}
# Calculate accuracy
confusionMatrix(as.factor(df$pred), df$X2)
```

Now it's time to build an ever so slightly more complex model, a logistic regression.

### Logistic regression

Let's build a logistic regression. You can read more about how logistic works [here](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed#logistic) and the instructor may show you some motivating and/or explanatory equations on the white/chalk-board. What's important to know is that _logistic regression_ is used for classification problems (such as our case of predicting whether a tumour is benign or malignant). **Important note:** logistic regression, or logreg, outputs a probability, which you'll then convert to a prediction.


Now build that logreg model:

```{r}
# Build model
model <- glm(X2 ~ ., family = "binomial", df)
# Predict probability on the same dataset
p <- predict(model, df, type="response")
# Convert probability to prediction "M" or "B"
pred <- ifelse(p > 0.50, "M", "B")

# Create confusion matrix
confusionMatrix(as.factor(pred), df$X2)
```

**Discussion:** From the above, can you say what the model accuracy is? 

_Also_, don't worry about the warnings. See [here for why](https://stackoverflow.com/questions/8596160/why-am-i-getting-algorithm-did-not-converge-and-fitted-prob-numerically-0-or).


_BUT_ this is the accuracy on the data that you trained the model on. This is not necessarily indicative of how the model will generalize to a dataset that it has never seen before, which is the purpose of building such models. For this reason, it is common to use a process called train test split to train the model on a subset of your data and then to compute the accuracy on the test set.

```{r}
# Set seed for reproducible results
set.seed(42)
# Train test split
inTraining <- createDataPartition(df$X2, p = .75, list=FALSE)
# Create train set
df_train <- df[ inTraining,]
# Create test set
df_test <- df[-inTraining,]
# Fit model to train set
model <- glm(X2 ~ ., family="binomial", df_train)
# Predict on test set
p <- predict(model, df_test, type="response")
pred <- ifelse(p > 0.50, "M", "B")

# Create confusion matrix
confusionMatrix(as.factor(pred), df_test$X2)
```


### Random Forests

This caret API is so cool you can use it for lots of models. You'll build random forests below. Before describing random forests, you'll need to know a bit about decision tree classifiers. It is a tree that allows you to classify data points (also known as "target variables", for example, benign or malignant tumor) based on feature variables (such as geometric measurements of tumors). See [here](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1519834394/bc_fdf2rr.png) for an example. The depth of the tree is known as a hyperparameter, which means a parameter you need to decide before you fit the model to the data. You can read more about decision trees [here](https://www.datacamp.com/community/tutorials/kaggle-tutorial-machine-learning). A _random forest_ is a collection of decision trees that fits different decision trees with different subsets of the data and gets them to vote on the label. This provides intuition behind random forests and you can find more technical details [here](https://en.wikipedia.org/wiki/Random_forest).

Before you build your first random forest, there's a pretty cool alternative to train test split called k-fold cross validation that we'll look into.


#### Cross Validation

To choose your random forest hyperparameter `max_depth`, for example, you'll use a variation on test train split called cross validation.

We begin by splitting the dataset into 5 groups or _folds_ (see [here](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303215/cv_raxrt7.png), for example). Then we hold out the first fold as a test set, fit our model on the remaining four folds, predict on the test set and compute the metric of interest. Next we hold out the second fold as our test set, fit on the remaining data, predict on the test set and compute the metric of interest. Then similarly with the third, fourth and fifth.

As a result we get five values of accuracy, from which we can compute statistics of interest, such as the median and/or mean and 95% confidence intervals.

We do this for each value of each hyperparameter that we're tuning and choose the set of hyperparameters that performs the best. This is called _grid search_ if we specify the hyperparameter values we wish to try and called _random search_, which searched randomly through the hyperparameter space (see more [here](http://topepo.github.io/caret/random-hyperparameter-search.html)).

You'll first build a random forest with a grid containing 1 hyperparameter to get a feel for it.

```{r}
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- sqrt(ncol(df))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(X2~., data=df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

Now try your hand at a random search:

```{r}
# Random Search
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="random")
mtry <- sqrt(ncol(df))
rf_random <- train(X2~., data=df, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)

```

And plot the results:

```{r}
plot(rf_random)
```

## Supervised Learning I: regression

In the classification task above, we were attempting to predict a categorical outcome, in this case 'benign' or 'malignant'. Regression, the other type of supervised learning task, is one in which you're attempting to predict a continuously varying outcome, such as the price of a house or life expectancy.

```{r}
gm <- read_csv("data/gapminder.csv") 
gm %>% head()
```


Plot life expectancy as a function of fertility:


```{r}
ggplot(gm, aes(x=fertility, y=life)) + geom_point()
```


**Discussion**: what type of regression model might be useful for modeling the above relationship?



Now you'll build a linear model for the relationship between life expectancy and fertility. For more on the math of linear models, see [here](https://www.datacamp.com/community/tutorials/data-science-techniques-dataframed#linear).

```{r}
mod <- lm(life~fertility, gm)
pred <- predict(mod, gm)
```

Plot the original data, along with the linear regression:


```{r}
{plot(gm$fertility, gm$life)
abline(mod)}
```

**Discussion:** Many data scientists and statisticians really dig linear regression over more complex models, often citing the reason that it is interpretable: what could this mean?


### Compute error

What linear regression does when fitting the line to the data is it minimizes the root mean square error (RMSE). Well, it actually minimizes the mean square error but these amount to the same thing. Compute the RMSE of your linear regression model:


```{r}
er <- pred - gm$life
rmse <- sqrt(mean(er^2))
rmse
```


Now you will build a full linear regression model, using all the variables that are in the dataset:

```{r}
mod_full <- lm(life~., gm)
pred_full <- predict(mod_full, gm)
er_full <- pred_full - gm$life
rmse_full <- sqrt(mean(er_full^2))
rmse_full
```


But recall that this may not signify the RMSE on a new dataset that the model has not seen. For this reason, you'll perform a test train split and compute the RMSE:

```{r}
# Set seed for reproducible results
set.seed(42)
# Train test split
inTraining <- createDataPartition(gm$life, p = .75, list=FALSE)
# Create train set
gm_train <- gm[ inTraining,]
# Create test set
gm_test <- gm[-inTraining,]
# Fit model to train set
model <- lm(life ~ ., gm_train)
# Predict on test set
p <- predict(model, gm_test)

#
er <- p - gm_test$life
rmse <- sqrt(mean(er^2))
rmse
```

## Supervised Learning II: regularized regression

When performing linear regression, instead of minimizing the MSE, you can add other constraints that will stop the model parameters from shooting up too high. Lasso regression and ridge regression are examples of these. Your instructor will write several equations on the board to explain these constraints and all the necessary information is also in [this glmnet vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html). You'll use the glmnet package to fit a lasso regression to the data:


```{r}
x = as.matrix(subset(gm, select=-life))
y = gm$life
fit = glmnet(x, y)
plot(fit, label=TRUE)
```

**Discussion:** Interpret the above figure. For a hint, check out [this part of Hastie & Qian's vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#qs):

### Lasso regression and cross validation

The glmnet API makes k-fold cross validation pretty easy. Give it a go and find the best choice for the hyperparameter lambda:

```{r}
cvfit = cv.glmnet(x, y, alph=1)
print(cvfit$lambda.min)
plot(cvfit)
```

### Feature selection using lasso regression

One great of aspect of lasso regression is that it can be used for automatic feature selection. Once you have used k-fold CV to find the best lambda, you can look at the coefficients of each variable (for that value of lambda) and the variables with the largest coefficients are the ones to select.




```{r}
x = as.matrix(subset(gm, select=-life))
y = gm$life
fit = glmnet(x, y, alpha=1)
plot(fit, label=TRUE)
```


** Discussion:** 

1. What is the most important variable in the above?
2. Why would automatic variable selection be useful?
3. What are potential pitfalls of automatic variable selection?

